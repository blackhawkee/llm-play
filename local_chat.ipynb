{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOevEEpzibt3W6blre/ls5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhawkee/llm-play/blob/main/local_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3XKPMAEIlcL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/blackhawkee/localGPT.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4kuLBf9vI-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/localGPT/requirements.txt"
      ],
      "metadata": {
        "id": "2U-E-DsJJSK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PanQiWei/AutoGPTQ.git\n",
        "%cd /content/AutoGPTQ\n",
        "!git checkout v0.2.2\n",
        "!pip install .\n",
        "!pip install fake_useragent"
      ],
      "metadata": {
        "id": "FCcyHq8WJcS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "ua_str = UserAgent().chrome\n",
        "\n",
        "file_url = \"https://constitutioncenter.org/media/files/constitution.pdf\"\n",
        "\n",
        "response = requests.get(file_url, headers={\"User-Agent\": ua_str}, stream=False)\n",
        "\n",
        "print(response.status_code)\n",
        "\n",
        "with open(\"/content/localGPT/SOURCE_DOCUMENTS/constitution.pdf\", \"wb\") as file:\n",
        "\tfile.write(response.content)\n"
      ],
      "metadata": {
        "id": "jSCANkwlJ9pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/localGPT/ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCf3BARAKWAw",
        "outputId": "199edb49-32a1-4c0d-aff9-c32a308c761b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-23 14:59:06,414 - INFO - ingest.py:120 - Loading documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-07-23 14:59:06,424 - INFO - ingest.py:33 - Loading document batch\n",
            "2023-07-23 14:59:09,855 - INFO - ingest.py:129 - Loaded 1 documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-07-23 14:59:09,855 - INFO - ingest.py:130 - Split into 72 chunks of text\n",
            "2023-07-23 14:59:13,715 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "2023-07-23 14:59:14,318 - INFO - instantiator.py:21 - Created a temporary directory at /tmp/tmpmlnzdkb6\n",
            "2023-07-23 14:59:14,318 - INFO - instantiator.py:76 - Writing /tmp/tmpmlnzdkb6/_remote_module_non_scriptable.py\n",
            "2023-07-23 14:59:15.747985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "max_seq_length  512\n",
            "2023-07-23 14:59:20,508 - INFO - __init__.py:88 - Running Chroma using direct local API.\n",
            "2023-07-23 14:59:20,520 - WARNING - __init__.py:43 - Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "2023-07-23 14:59:20,526 - INFO - ctypes.py:22 - Successfully imported ClickHouse Connect C data optimizations\n",
            "2023-07-23 14:59:20,533 - INFO - json_impl.py:45 - Using python library for writing JSON byte strings\n",
            "2023-07-23 14:59:20,578 - INFO - duckdb.py:460 - loaded in 72 embeddings\n",
            "2023-07-23 14:59:20,579 - INFO - duckdb.py:472 - loaded in 1 collections\n",
            "2023-07-23 14:59:20,580 - INFO - duckdb.py:89 - collection with name langchain already exists, returning existing collection\n",
            "2023-07-23 14:59:27,379 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /content/localGPT/DB\n",
            "2023-07-23 14:59:27,430 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /content/localGPT/DB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/localGPT/run_localGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyA5ak-6QVCQ",
        "outputId": "324f27ca-7907-49b4-f64f-3df310eec53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-23 15:00:26.527684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-07-23 15:00:31,027 - INFO - run_localGPT.py:176 - Running on: cuda\n",
            "2023-07-23 15:00:31,027 - INFO - run_localGPT.py:177 - Display Source Documents set to: False\n",
            "2023-07-23 15:00:31,211 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "2023-07-23 15:00:35,127 - INFO - __init__.py:88 - Running Chroma using direct local API.\n",
            "2023-07-23 15:00:35,140 - WARNING - __init__.py:43 - Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "2023-07-23 15:00:35,146 - INFO - ctypes.py:22 - Successfully imported ClickHouse Connect C data optimizations\n",
            "2023-07-23 15:00:35,154 - INFO - json_impl.py:45 - Using python library for writing JSON byte strings\n",
            "2023-07-23 15:00:35,201 - INFO - duckdb.py:460 - loaded in 144 embeddings\n",
            "2023-07-23 15:00:35,202 - INFO - duckdb.py:472 - loaded in 1 collections\n",
            "2023-07-23 15:00:35,203 - INFO - duckdb.py:89 - collection with name langchain already exists, returning existing collection\n",
            "2023-07-23 15:00:35,203 - INFO - run_localGPT.py:43 - Loading Model: TheBloke/Llama-2-7B-Chat-GGML, on: cuda\n",
            "2023-07-23 15:00:35,203 - INFO - run_localGPT.py:44 - This action can take a few minutes!\n",
            "2023-07-23 15:00:35,203 - INFO - run_localGPT.py:64 - Using AutoGPTQForCausalLM for quantized models\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/localGPT/run_localGPT.py\", line 256, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/content/localGPT/run_localGPT.py\", line 225, in main\n",
            "    llm = load_model(device_type, model_id=model_id, model_basename=model_basename)\n",
            "  File \"/content/localGPT/run_localGPT.py\", line 70, in load_model\n",
            "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 720, in from_pretrained\n",
            "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 1825, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load tokenizer for 'TheBloke/Llama-2-7B-Chat-GGML'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7B-Chat-GGML' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\n",
            "2023-07-23 15:00:37,235 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /content/localGPT/DB\n"
          ]
        }
      ]
    }
  ]
}