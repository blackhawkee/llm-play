{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvIihloMxgO3E2lQbWbMxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhawkee/llm-play/blob/main/LaMini_T5_738M_pdf_chat_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLSii8UP9rTL",
        "outputId": "1d70c4ce-501a-4bee-d619-8385d3d41156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/681.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.3/681.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain streamlit transformers requests torch einops accelerate bitsandbytes pdfminer.six bs4 sentence_transformers chromadb==0.3.29 pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p docs & wget -P /content/docs/ https://github.com/AIAnytime/Search-Your-PDF-App/blob/main/docs/fastfacts-what-is-climate-change.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYb5eq87-rfW",
        "outputId": "c491c170-699f-4aa7-df37-586fc1dfb229"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-02 16:08:54--  https://github.com/AIAnytime/Search-Your-PDF-App/blob/main/docs/fastfacts-what-is-climate-change.pdf\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 891187 (870K) [text/plain]\n",
            "Saving to: ‚Äò/content/docs/fastfacts-what-is-climate-change.pdf‚Äô\n",
            "\n",
            "fastfacts-what-is-c 100%[===================>] 870.30K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-08-02 16:08:55 (24.5 MB/s) - ‚Äò/content/docs/fastfacts-what-is-climate-change.pdf‚Äô saved [891187/891187]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile constants.py\n",
        "import os\n",
        "from chromadb.config import Settings\n",
        "\n",
        "#Define the chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',\n",
        "    persist_directory = \"db\",\n",
        "    anonymized_telemetry = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lix08c0g93-w",
        "outputId": "eeb5c308-34b1-4757-8920-1e32ed0f0297"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing constants.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ingest.py\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "from constants import CHROMA_SETTINGS\n",
        "\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    for root, dirs, files in os.walk(\"/content/docs\"):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                print(file)\n",
        "                loader = PDFMinerLoader(os.path.join(root, file))\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    #create embeddings here\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "    db.persist()\n",
        "    db=None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5-kGPo-YNc",
        "outputId": "e835d3e1-2c91-4429-9fcc-71da635cfa20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ingest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import base64\n",
        "import textwrap\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from constants import CHROMA_SETTINGS\n",
        "\n",
        "#model and tokenizer loading\n",
        "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='auto', torch_dtype=torch.float32)\n",
        "\n",
        "@st.cache_resource\n",
        "def llm_pipeline():\n",
        "    pipe = pipeline(\n",
        "        'text2text-generation',\n",
        "        model = base_model,\n",
        "        tokenizer = tokenizer,\n",
        "        max_length = 256,\n",
        "        do_sample=True,\n",
        "        temperature = 0.3,\n",
        "        top_p = 0.95\n",
        "    )\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return local_llm\n",
        "\n",
        "@st.cache_resource\n",
        "def qa_llm():\n",
        "    llm = llm_pipeline()\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma(persist_directory=\"db\", embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "    retriever = db.as_retriever()\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "    return qa\n",
        "\n",
        "def process_answer(instruction):\n",
        "    response = ''\n",
        "    instruction = instruction\n",
        "    qa = qa_llm()\n",
        "    generated_text = qa(instruction)\n",
        "    answer = generated_text['result']\n",
        "    # metadata = generated_text['metadata']\n",
        "    # for text in generated_text:\n",
        "\n",
        "    #     print(answer)\n",
        "\n",
        "    # wrapped_text = textwrap.fill(response, 100)\n",
        "    # return wrapped_text\n",
        "    return answer,generated_text\n",
        "\n",
        "def main():\n",
        "    st.title(\"Search Your PDF üê¶üìÑ\")\n",
        "    with st.expander(\"About the App\"):\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            This is a Generative AI powered Question and Answering app that responds to questions about your PDF File.\n",
        "            \"\"\"\n",
        "        )\n",
        "    question = st.text_area(\"Enter your Question\")\n",
        "    if st.button(\"Ask\"):\n",
        "        st.info(\"Your Question: \" + question)\n",
        "\n",
        "        st.info(\"Your Answer\")\n",
        "        answer, metadata = process_answer(question)\n",
        "        st.write(answer)\n",
        "        st.write(metadata)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCMsfxQL-jVt",
        "outputId": "74786d70-9b43-4b90-829f-d1ac274457dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Run the app\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SIkzOTkB-7X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bHvokBt_jud",
        "outputId": "6d79069c-204f-4510-e15d-acd249f2e12b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fastfacts-what-is-climate-change.pdf\n",
            "2023-08-02 16:16:42.127819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "tnhdDw7Q-_-b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "# Setup the ngrok tunnel to the Streamlit app\n",
        "!ngrok config add-authtoken 2TLdGJHytafVAyzDiZTsmQ8DHSg_84gTAQVReZ3duhLYMpLwu\n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHuTl5P_Cyz",
        "outputId": "27127ed8-9b80-4b7c-d1e4-20290d946fbb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-02T16:33:17+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://8e94-34-87-69-251.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "Xi9iUKys_Eb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}