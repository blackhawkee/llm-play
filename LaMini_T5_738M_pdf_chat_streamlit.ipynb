{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhawkee/llm-play/blob/main/LaMini_T5_738M_pdf_chat_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLSii8UP9rTL",
        "outputId": "171ed7fd-8574-4b2c-acde-75430ca5e865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain streamlit streamlit_chat transformers==4.30.2 requests torch einops accelerate bitsandbytes pdfminer.six bs4 sentence_transformers chromadb==0.3.29 pyngrok fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "directory = \"/content/docs/\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "ua = UserAgent()\n",
        "header = {'User-Agent':str(ua.chrome)}\n",
        "\n",
        "file_url = 'https://www.un.org/sites/un2.un.org/files/fastfacts-what-is-climate-change.pdf'\n",
        "response = requests.get(file_url, headers=header)\n",
        "print(response)\n",
        "\n",
        "with open(\"/content/docs/fastfacts-what-is-climate-change.pdf\", \"wb\") as file:\n",
        "  file.write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUkM6VBYt022",
        "outputId": "2768eeb2-d0dc-4240-bf91-f9961e5fffa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lix08c0g93-w",
        "outputId": "b7991f3b-dbc9-41ad-a46c-ffe91f68bbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing constants.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile constants.py\n",
        "import os\n",
        "from chromadb.config import Settings\n",
        "\n",
        "#Define the chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',\n",
        "    persist_directory = \"db\",\n",
        "    anonymized_telemetry = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5-kGPo-YNc",
        "outputId": "418fd0d2-f3b7-4c24-a936-3794eb1653db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ingest.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ingest.py\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "import tempfile\n",
        "from constants import CHROMA_SETTINGS\n",
        "\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    for root, dirs, files in os.walk(\"/content/docs\"):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                print(file)\n",
        "                loader = PDFMinerLoader(os.path.join(root, file))\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    #create embeddings here\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "    db.persist()\n",
        "    db=None\n",
        "\n",
        "def upload_pdf(content):\n",
        "  with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
        "        tmp_file.write(content)\n",
        "        tmp_file_path = tmp_file.name\n",
        "        print(tmp_file_path)\n",
        "        loader = PDFMinerLoader(tmp_file_path)\n",
        "\n",
        "  documents = loader.load()\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "  #create embeddings here\n",
        "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "  db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "  db.persist()\n",
        "  return db\n",
        "  #db=None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCMsfxQL-jVt",
        "outputId": "7283736e-11c3-48e6-bfa0-1b153dd70427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import base64\n",
        "import textwrap\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from constants import CHROMA_SETTINGS\n",
        "#from transformers import logging\n",
        "import logging\n",
        "from ingest import upload_pdf\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from streamlit_chat import message\n",
        "\n",
        "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
        "#logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "#model and tokenizer loading\n",
        "#hg_model = \"MBZUAI/LaMini-T5-738M\"\n",
        "hg_model = \"hkunlp/instructor-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(hg_model)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(hg_model, device_map='auto', torch_dtype=torch.float32)\n",
        "\n",
        "#sentence_transformer_model_name = \"all-MiniLM-L6-v2\"\n",
        "sentence_transformer_model_name = \"all-mpnet-base-v22\"\n",
        "\n",
        "@st.cache_resource\n",
        "def llm_pipeline():\n",
        "    pipe = pipeline(\n",
        "        'text2text-generation',\n",
        "        model = base_model,\n",
        "        tokenizer = tokenizer,\n",
        "        max_length = 256,\n",
        "        do_sample=True,\n",
        "        temperature = 0.3,\n",
        "        top_p = 0.95\n",
        "    )\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return local_llm\n",
        "\n",
        "@st.cache_resource\n",
        "def qa_llm():\n",
        "    llm = llm_pipeline()\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=sentence_transformer_model_name)\n",
        "    db = Chroma(persist_directory=\"db\", embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "    retriever = db.as_retriever()\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "    return qa\n",
        "\n",
        "def process_answer(instruction):\n",
        "    response = ''\n",
        "    instruction = instruction\n",
        "    qa = qa_llm()\n",
        "    generated_text = qa(instruction)\n",
        "    answer = generated_text['result']\n",
        "    # metadata = generated_text['metadata']\n",
        "    # for text in generated_text:\n",
        "\n",
        "    #     print(answer)\n",
        "\n",
        "    # wrapped_text = textwrap.fill(response, 100)\n",
        "    # return wrapped_text\n",
        "    return answer,generated_text\n",
        "\n",
        "def main():\n",
        "    st.title(\"Converse with Your PDF \\U0001F917 üê¶üìÑ\")\n",
        "    with st.expander(\"About the App\"):\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            This is a Generative AI powered Question and Answering app that responds to questions about your PDF File.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Upload your Data\", type=\"pdf\")\n",
        "    if uploaded_file :\n",
        "        db = upload_pdf(uploaded_file.getvalue())\n",
        "        llm = llm_pipeline()\n",
        "\n",
        "        chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever())\n",
        "\n",
        "        def conversational_chat(query):\n",
        "            result = chain({\"question\": query, \"chat_history\": st.session_state['history']})\n",
        "            st.session_state['history'].append((query, result[\"answer\"]))\n",
        "            return result[\"answer\"]\n",
        "\n",
        "        if 'history' not in st.session_state:\n",
        "            st.session_state['history'] = []\n",
        "\n",
        "        if 'generated' not in st.session_state:\n",
        "            st.session_state['generated'] = [\"Hello ! Ask me anything about \" + uploaded_file.name + \" ü§ó\"]\n",
        "\n",
        "        if 'past' not in st.session_state:\n",
        "            st.session_state['past'] = [\"Hey ! üëã\"]\n",
        "\n",
        "        #container for the chat history\n",
        "        response_container = st.container()\n",
        "        #container for the user's text input\n",
        "        container = st.container()\n",
        "\n",
        "        with container:\n",
        "            with st.form(key='my_form', clear_on_submit=True):\n",
        "\n",
        "                user_input = st.text_input(\"Query:\", placeholder=\"Talk to your pdf data here <:>\", key='input')\n",
        "                submit_button = st.form_submit_button(label='Send')\n",
        "\n",
        "            if submit_button and user_input:\n",
        "                output = conversational_chat(user_input)\n",
        "\n",
        "                st.session_state['past'].append(user_input)\n",
        "                st.session_state['generated'].append(output)\n",
        "\n",
        "        if st.session_state['generated']:\n",
        "            with response_container:\n",
        "                for i in range(len(st.session_state['generated'])):\n",
        "                    message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"big-smile\")\n",
        "                    message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"thumbs\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIkzOTkB-7X1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Run the app\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bHvokBt_jud"
      },
      "outputs": [],
      "source": [
        "!#python /content/ingest.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tnhdDw7Q-_-b"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/dev/null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHuTl5P_Cyz",
        "outputId": "0f0475f9-06e4-48a9-9882-917520283063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-07T12:57:25+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://6f4d-34-91-5-208.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "!ngrok config add-authtoken 2TLdGJHytafVAyzDiZTsmQ8DHSg_84gTAQVReZ3duhLYMpLwu\n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xi9iUKys_Eb8"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "import torch\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/db"
      ],
      "metadata": {
        "id": "A0qzPmTpJaSj"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}