{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhawkee/llm-play/blob/main/LaMini_T5_738M_pdf_chat_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLSii8UP9rTL"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain streamlit transformers==4.30.2 requests torch einops accelerate bitsandbytes pdfminer.six bs4 sentence_transformers chromadb==0.3.29 pyngrok fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "directory = \"/content/docs/\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "ua = UserAgent()\n",
        "header = {'User-Agent':str(ua.chrome)}\n",
        "\n",
        "file_url = 'https://www.un.org/sites/un2.un.org/files/fastfacts-what-is-climate-change.pdf'\n",
        "response = requests.get(file_url, headers=header)\n",
        "print(response)\n",
        "\n",
        "with open(\"/content/docs/fastfacts-what-is-climate-change.pdf\", \"wb\") as file:\n",
        "  file.write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUkM6VBYt022",
        "outputId": "e48e9a18-1d01-427f-fe2c-d205fe0d637d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lix08c0g93-w",
        "outputId": "e3eca972-f2d3-467e-8343-d2eac76b7cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing constants.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile constants.py\n",
        "import os\n",
        "from chromadb.config import Settings\n",
        "\n",
        "#Define the chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',\n",
        "    persist_directory = \"db\",\n",
        "    anonymized_telemetry = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5-kGPo-YNc",
        "outputId": "bdaec02a-dd8d-4d02-dadd-2e49de708b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ingest.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ingest.py\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "from constants import CHROMA_SETTINGS\n",
        "\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    for root, dirs, files in os.walk(\"/content/docs\"):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                print(file)\n",
        "                loader = PDFMinerLoader(os.path.join(root, file))\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    #create embeddings here\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "    db.persist()\n",
        "    db=None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCMsfxQL-jVt",
        "outputId": "15dc2f92-612a-4b02-fb95-aaadfd5ef7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import base64\n",
        "import textwrap\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from constants import CHROMA_SETTINGS\n",
        "#from transformers import logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
        "#logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "#model and tokenizer loading\n",
        "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='auto', torch_dtype=torch.float32)\n",
        "\n",
        "@st.cache_resource\n",
        "def llm_pipeline():\n",
        "    pipe = pipeline(\n",
        "        'text2text-generation',\n",
        "        model = base_model,\n",
        "        tokenizer = tokenizer,\n",
        "        max_length = 256,\n",
        "        do_sample=True,\n",
        "        temperature = 0.3,\n",
        "        top_p = 0.95\n",
        "    )\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return local_llm\n",
        "\n",
        "@st.cache_resource\n",
        "def qa_llm():\n",
        "    llm = llm_pipeline()\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma(persist_directory=\"db\", embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "    retriever = db.as_retriever()\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "    return qa\n",
        "\n",
        "def process_answer(instruction):\n",
        "    response = ''\n",
        "    instruction = instruction\n",
        "    qa = qa_llm()\n",
        "    generated_text = qa(instruction)\n",
        "    answer = generated_text['result']\n",
        "    # metadata = generated_text['metadata']\n",
        "    # for text in generated_text:\n",
        "\n",
        "    #     print(answer)\n",
        "\n",
        "    # wrapped_text = textwrap.fill(response, 100)\n",
        "    # return wrapped_text\n",
        "    return answer,generated_text\n",
        "\n",
        "def main():\n",
        "    st.title(\"Search Your PDF üê¶üìÑ\")\n",
        "    with st.expander(\"About the App\"):\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            This is a Generative AI powered Question and Answering app that responds to questions about your PDF File.\n",
        "            \"\"\"\n",
        "        )\n",
        "    question = st.text_area(\"Enter your Question\")\n",
        "    if st.button(\"Ask\"):\n",
        "        st.info(\"Your Question: \" + question)\n",
        "\n",
        "        st.info(\"Your Answer\")\n",
        "        answer, metadata = process_answer(question)\n",
        "        st.write(answer)\n",
        "        st.write(metadata)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIkzOTkB-7X1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Run the app\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bHvokBt_jud",
        "outputId": "98d9a00d-94c4-424c-a9ef-4d18b6f6dc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fastfacts-what-is-climate-change.pdf\n",
            "Downloading (‚Ä¶)e9125/.gitattributes: 100% 1.18k/1.18k [00:00<00:00, 7.33MB/s]\n",
            "Downloading (‚Ä¶)_Pooling/config.json: 100% 190/190 [00:00<00:00, 1.26MB/s]\n",
            "Downloading (‚Ä¶)7e55de9125/README.md: 100% 10.6k/10.6k [00:00<00:00, 47.1MB/s]\n",
            "Downloading (‚Ä¶)55de9125/config.json: 100% 612/612 [00:00<00:00, 3.04MB/s]\n",
            "Downloading (‚Ä¶)ce_transformers.json: 100% 116/116 [00:00<00:00, 732kB/s]\n",
            "Downloading (‚Ä¶)125/data_config.json: 100% 39.3k/39.3k [00:00<00:00, 9.58MB/s]\n",
            "Downloading pytorch_model.bin: 100% 90.9M/90.9M [00:00<00:00, 123MB/s]\n",
            "Downloading (‚Ä¶)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 343kB/s]\n",
            "Downloading (‚Ä¶)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 702kB/s]\n",
            "Downloading (‚Ä¶)e9125/tokenizer.json: 100% 466k/466k [00:00<00:00, 7.85MB/s]\n",
            "Downloading (‚Ä¶)okenizer_config.json: 100% 350/350 [00:00<00:00, 1.67MB/s]\n",
            "Downloading (‚Ä¶)9125/train_script.py: 100% 13.2k/13.2k [00:00<00:00, 47.4MB/s]\n",
            "Downloading (‚Ä¶)7e55de9125/vocab.txt: 100% 232k/232k [00:00<00:00, 11.4MB/s]\n",
            "Downloading (‚Ä¶)5de9125/modules.json: 100% 349/349 [00:00<00:00, 2.10MB/s]\n",
            "2023-08-07 11:54:49.894639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "!python /content/ingest.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tnhdDw7Q-_-b"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/dev/null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHuTl5P_Cyz",
        "outputId": "ac873d43-16f7-47a8-8185-bd54e4092d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-07T12:01:45+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://359f-34-139-188-195.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "!ngrok config add-authtoken 2TLdGJHytafVAyzDiZTsmQ8DHSg_84gTAQVReZ3duhLYMpLwu\n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xi9iUKys_Eb8"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "import torch\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}