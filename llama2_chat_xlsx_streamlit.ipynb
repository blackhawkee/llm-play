{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhawkee/llm-play/blob/main/llama2_chat_xlsx_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok pypdf langchain torch accelerate bitsandbytes transformers sentence_transformers faiss_cpu streamlit_chat ctransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaXsFhEFRsC3",
        "outputId": "2a5637b4-2223-419c-fa54-7e731dedc559"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p model & wget -P /content/model/ https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cieyHZmOV7iV",
        "outputId": "ccfc5641-22ca-4500-b6d0-637b39737a68"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-31 19:09:28--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.94.52, 108.138.94.97, 108.138.94.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.94.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691089768&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTA4OTc2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=Qad7P5V8rDtz2JgQIEGyR-k1TxHdyKVAm1q4eWCoiv5khX9CBsPT4e8IPhxoc%7E9lVRTc3ptk4jCaEYLcloIDHkmisMzfZzf8d51NynGvqWuDd%7EurPtUPsxUDkn5YhhfJChH-3elNn%7EtSt7OnUg3k0ogf9PCDkBxhTEU6mGVc%7EdlbZoSuUQ51piD4qgfJzYfrGYge3yhyqvUJxAq3IkD1xMs1m%7E9OxtgIrGmt-bv38EFcBGAzIq9nGGhpg1%7EeueRIlbmNjLAdUIvHU38gfJQBrilTFEudOpINfUJhG362YjLjQWvh%7E16QHVkgqIHpsVDJAenMgrD6u1qby%7EhcpabLEg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-07-31 19:09:28--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691089768&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTA4OTc2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=Qad7P5V8rDtz2JgQIEGyR-k1TxHdyKVAm1q4eWCoiv5khX9CBsPT4e8IPhxoc%7E9lVRTc3ptk4jCaEYLcloIDHkmisMzfZzf8d51NynGvqWuDd%7EurPtUPsxUDkn5YhhfJChH-3elNn%7EtSt7OnUg3k0ogf9PCDkBxhTEU6mGVc%7EdlbZoSuUQ51piD4qgfJzYfrGYge3yhyqvUJxAq3IkD1xMs1m%7E9OxtgIrGmt-bv38EFcBGAzIq9nGGhpg1%7EeueRIlbmNjLAdUIvHU38gfJQBrilTFEudOpINfUJhG362YjLjQWvh%7E16QHVkgqIHpsVDJAenMgrD6u1qby%7EhcpabLEg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.35, 18.65.229.73, 18.65.229.16, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘/content/model/llama-2-7b-chat.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   6.67G   140MB/s    in 92s     \n",
            "\n",
            "2023-07-31 19:11:00 (74.3 MB/s) - ‘/content/model/llama-2-7b-chat.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "import tempfile\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import logging as log\n",
        "\n",
        "log.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "DB_FAISS_PATH = '/content/vectorstore/db_faiss'\n",
        "\n",
        "#Loading the model\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    log.info('loading the llm model')\n",
        "    llm = CTransformers(\n",
        "        model = \"/content/model/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens = 512,\n",
        "        temperature = 0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def load_gptq_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    log.info('loading the gptq llm model')\n",
        "    llm = CTransformers(\n",
        "        model = \"/content/model/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens = 512,\n",
        "        temperature = 0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "st.title(\"Chat with CSV using Llama2 🦙🦜\")\n",
        "# st.markdown(\"<h3 style='text-align: center; color: white;'>Built by <a href='https://github.com/AIAnytime'>AI Anytime with ❤️ </a></h3>\", unsafe_allow_html=True)\n",
        "\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload your Data\", type=\"csv\")\n",
        "\n",
        "if uploaded_file :\n",
        "   #use tempfile because CSVLoader only accepts a file_path\n",
        "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    loader = CSVLoader(file_path=tmp_file_path, encoding=\"utf-8\", csv_args={\n",
        "                'delimiter': ','})\n",
        "    data = loader.load()\n",
        "    #st.json(data)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cuda'})\n",
        "\n",
        "    db = FAISS.from_documents(data, embeddings)\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "    llm = load_llm()\n",
        "    chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever())\n",
        "\n",
        "    def conversational_chat(query):\n",
        "        result = chain({\"question\": query, \"chat_history\": st.session_state['history']})\n",
        "        st.session_state['history'].append((query, result[\"answer\"]))\n",
        "        return result[\"answer\"]\n",
        "\n",
        "    if 'history' not in st.session_state:\n",
        "        st.session_state['history'] = []\n",
        "\n",
        "    if 'generated' not in st.session_state:\n",
        "        st.session_state['generated'] = [\"Hello ! Ask me anything about \" + uploaded_file.name + \" 🤗\"]\n",
        "\n",
        "    if 'past' not in st.session_state:\n",
        "        st.session_state['past'] = [\"Hey ! 👋\"]\n",
        "\n",
        "    #container for the chat history\n",
        "    response_container = st.container()\n",
        "    #container for the user's text input\n",
        "    container = st.container()\n",
        "\n",
        "    with container:\n",
        "        with st.form(key='my_form', clear_on_submit=True):\n",
        "\n",
        "            user_input = st.text_input(\"Query:\", placeholder=\"Talk to your csv data here (:\", key='input')\n",
        "            submit_button = st.form_submit_button(label='Send')\n",
        "\n",
        "        if submit_button and user_input:\n",
        "            output = conversational_chat(user_input)\n",
        "\n",
        "            st.session_state['past'].append(user_input)\n",
        "            st.session_state['generated'].append(output)\n",
        "\n",
        "    if st.session_state['generated']:\n",
        "        with response_container:\n",
        "            for i in range(len(st.session_state['generated'])):\n",
        "                message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"big-smile\")\n",
        "                message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"thumbs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNoo-i5RtwY",
        "outputId": "eec48ce2-b517-43ec-a416-f7ed30b4277b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Run the above code**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tavK3BV4U9cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "Lh70mSbdRxam"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "# Setup the ngrok tunnel to the Streamlit app\n",
        "!ngrok config add-authtoken 2TLdGJHytafVAyzDiZTsmQ8DHSg_84gTAQVReZ3duhLYMpLwu\n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDP_0JhR0EW",
        "outputId": "ebdc7deb-c63f-4e8c-d36d-ad05834a2d4d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-31T19:27:49+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://aaf3-104-196-235-13.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "yGnrV_8CUiw-"
      },
      "execution_count": 40,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHQnuPKv/o+WdwFGPFQh9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}